{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "MHNC_TH9_18110014.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Máy học nâng cao - Lab 09\n",
        "# MSSV: 18110014 - Họ tên: Nguyễn Phú Thành"
      ],
      "metadata": {
        "id": "MBGaVAHGB_aY"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DdONwSgab2Rp",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "779e0616-3cec-4989-a01f-8a86d1e0e1f5"
      },
      "source": [
        "!pip install gym"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: gym in /usr/local/lib/python3.7/dist-packages (0.17.3)\n",
            "Requirement already satisfied: cloudpickle<1.7.0,>=1.2.0 in /usr/local/lib/python3.7/dist-packages (from gym) (1.3.0)\n",
            "Requirement already satisfied: pyglet<=1.5.0,>=1.4.0 in /usr/local/lib/python3.7/dist-packages (from gym) (1.5.0)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.7/dist-packages (from gym) (1.4.1)\n",
            "Requirement already satisfied: numpy>=1.10.4 in /usr/local/lib/python3.7/dist-packages (from gym) (1.19.5)\n",
            "Requirement already satisfied: future in /usr/local/lib/python3.7/dist-packages (from pyglet<=1.5.0,>=1.4.0->gym) (0.16.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jN49TjYeb-q7"
      },
      "source": [
        "import numpy as np\n",
        "import gym\n",
        "import time\n",
        "from IPython.display import clear_output"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "syWCk4KjcDIQ"
      },
      "source": [
        "env = gym.make(\"Taxi-v3\")"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v6ClA4m-carz"
      },
      "source": [
        "Chúng ta sẽ nói về một environment đơn giản, có số state và số action hữu hạn (và khá nhỏ) là Taxi-v2. Trong environment này, agent của chúng ta đóng vai trò 1 tài xế taxi. Có 4 địa điểm cố định khác nhau trên bản đồ (được ký hiệu R, G, Y, B), và mỗi khi environment bắt đầu, sẽ có 2 điểm bất kỳ là điểm đón và trả khách (2 điểm này có thể trùng nhau), cũng như vị trí của taxi cũng là vị trí bất kỳ. Nhiệm vụ của chúng ta là đón hành khách (ở điểm màu xanh da trời) và trả khách (ở điểm màu tím).\n",
        "\n",
        "Agent của chúng ta có thể thực hiện 6 actions:\n",
        "\n",
        "0: xuống dưới\n",
        "\n",
        "1: lên trên\n",
        "\n",
        "2: sang trái\n",
        "\n",
        "3: sang phải\n",
        "\n",
        "4: đón khách\n",
        "\n",
        "5: trả khách\n",
        "\n",
        "Reward của environment này được tính như sau:\n",
        "\n",
        "cứ sau 1 time step (tức là khi xảy ra bất kỳ action nào), sẽ nhận -1 reward\n",
        "nhận được +20 reward nếu ta trả khách thành công (nghĩa là time step đó ta sẽ nhận +19 reward)\n",
        "nếu agent có hành vi đón khách và trả khách không hợp lệ, nhận -10 reward (nghĩa là time step đó ta sẽ nhận -11 reward).\n",
        "\n",
        "taxi sẽ hoạt động trên một khu vực 5x5, với 4 điểm trả khác và 5 địa điểm hành khách đang đứng nên số states sẽ là 5x5x5x4=500"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CLt5RXBbch_6"
      },
      "source": [
        "Hành khách có thể ở 5 điểm là R, G, Y, B và trong xe, màu xanh dương để chỉ địa điểm hành khách đang đứng chờ (nếu hành khác trong taxi thì xe sẽ màu xanh lá), màu tím để chỉ địa điểm cần chở khách đến"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sgblgvtccGHT",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "31483fe6-a840-4658-d19d-0d1868eb199e"
      },
      "source": [
        "env.env.s = env.encode(1,1,2,0)\n",
        "env.render()"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+---------+\n",
            "|\u001b[35mR\u001b[0m: | : :G|\n",
            "| :\u001b[43m \u001b[0m| : : |\n",
            "| : : : : |\n",
            "| | : | : |\n",
            "|\u001b[34;1mY\u001b[0m| : |B: |\n",
            "+---------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HQn6KZjccIZ0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1c4b5d8a-f122-47d3-d3f5-110756e84332"
      },
      "source": [
        "env.env.s = env.encode(1,1,4,0)\n",
        "env.render()"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+---------+\n",
            "|\u001b[35mR\u001b[0m: | : :G|\n",
            "| :\u001b[42m_\u001b[0m| : : |\n",
            "| : : : : |\n",
            "| | : | : |\n",
            "|Y| : |B: |\n",
            "+---------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hBpmaSezco5l"
      },
      "source": [
        "ta sẽ dùng env.encode(số_hàng,số_cột,hành_khách,điểm_trả_khách), với:\n",
        "* số_hàng thuộc [0,1,2,3,4]\n",
        "* số_cột thuộc  [0,1,2,3,4]\n",
        "* hành_khách: 0 là ở R, 1 là ở G, 2 là ở Y, 3 là ở B, 4 là trên xe\n",
        "* điểm_trả_khách: 0 là ở R, 1 là ở G, 2 là ở Y, 3 là ở B"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WuNSrC12cyIq"
      },
      "source": [
        "# Naive Learning\n",
        "Bây giờ ta sẽ học Q-table một cách ngây thơ :\n",
        "\n",
        "* Chọn hành động tốt nhất dựa vào những gì đã học\n",
        "* Nếu điểm bằng nhau thì chọn ngẫu nhiên\n",
        "* Q-table cập nhất với discount = 0 (không xem xét tương lai)\n",
        "\n",
        "Đầu tiên ta phải tạo ra một q-table là ma trận $size\\_state\\times size\\_action$"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Mg_s9XGPcTs1"
      },
      "source": [
        "state_size = env.observation_space.n\n",
        "action_size = env.action_space.n\n",
        "\n",
        "q_table = np.zeros((state_size, action_size))\n",
        "\n",
        "FILE_SAVE_Naive = \"q_table_naive.npy\"\n",
        "total_episodes = 5000      # Total episodes\n",
        "total_test_episodes = 100  # Total test episodes\n",
        "max_steps = 99             # Max steps per episode"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eQhJNR6OdInG",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c7285691-2044-48fb-fab4-9a1616fb41d0"
      },
      "source": [
        "for episode in range(total_episodes):\n",
        "    state = env.reset()   # reset lại môi trường\n",
        "    done = False          # đã hoàn thành trả khách hay chưa\n",
        "    clear_output(wait = True)\n",
        "    print(f\"Episode = {episode + 1}/{total_episodes} \")\n",
        "    for step in range(max_steps):\n",
        "        if  np.max(q_table[state]) == 0:         # nếu trong state q_value đều bằng 0 thì chọn đại 1 action\n",
        "            action = np.random.randint(0,action_size)\n",
        "        else:\n",
        "            action = np.argmax(q_table[state])  # chọn action có q_value lớn nhất\n",
        "\n",
        "        new_state, reward, done, _ = env.step(action)   # thực hiện hành động để nhận reward và state, action mới\n",
        "\n",
        "        q_table[state,action] += reward # cập nhật q_table\n",
        "        state = new_state\n",
        "        if done:       # nếu taxi có hành vi trả khách thì kết thúc episode\n",
        "            break"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Episode = 5000/5000 \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "x9GSmTLBiP2r"
      },
      "source": [
        "np.save(FILE_SAVE_Naive,q_table)"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1k3NyHcLidqi",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d4a1a613-7246-446d-93d7-9e50f68411a3"
      },
      "source": [
        "q_table = np.load(\"q_table_naive.npy\")\n",
        "\n",
        "rewards = []\n",
        "\n",
        "for episode in range(total_test_episodes):\n",
        "    state = env.reset()\n",
        "    step = 0\n",
        "    done = False\n",
        "    total_rewards = 0\n",
        "    \n",
        "\n",
        "    for step in range(max_steps):\n",
        "        print(\"****************************************************\")\n",
        "        print(\"EPISODE \", episode + 1)\n",
        "        \n",
        "        action = np.argmax(q_table[state,:])\n",
        "        new_state, reward, done, info = env.step(action)\n",
        "        env.render() # hiển thị trò chơi\n",
        "        total_rewards += reward\n",
        "        print(total_rewards)\n",
        "        clear_output(wait = True)\n",
        "        \n",
        "        if done:\n",
        "            rewards.append(total_rewards)\n",
        "            break\n",
        "        state = new_state\n",
        "env.close()\n",
        "print (\"Score over time: \" +  str(sum(rewards)/total_test_episodes))"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Score over time: 0.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wVOyO5BEjtIs"
      },
      "source": [
        "# Q-Learning\n",
        "\n",
        "Giá trị Q-value được cập nhật như sau:$$Q(s_t, a_t) = Q(s_t, a_t) + \\alpha [r_{t+1} + \\lambda \\max_{a}Q(s_{t+1}, a) - Q(s_t, a_t)]$$với $\\alpha$ là learning rate, $\\lambda$ là discount rate, $s_t$ là quan sát thời điểm $t$ và $r_{t+1}$ là phần thưởng sau khi thực hiện hành động $a_t$ với quan sát $s_t$.\n",
        "\n",
        "Ngoài ra agent sẽ ngẫu nhiên thực hiện exploration với xác suất nào đó ở những state ban đầu"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Raou1vepjxpx"
      },
      "source": [
        "env = gym.make(\"Taxi-v3\")\n",
        "state_size = env.observation_space.n\n",
        "action_size = env.action_space.n\n",
        "\n",
        "q_table = np.zeros((state_size, action_size))\n",
        "\n",
        "FILE_SAVE_Qlearning = \"q_table_qlearning.npy\"\n",
        "total_episodes = 5000       # Total episodes\n",
        "total_test_episodes = 100   # Total test episodes\n",
        "max_steps = 99              # Max steps per episode\n",
        "\n",
        "learning_rate = 0.7          # Learning rate\n",
        "discount_rate = 0.95         # Discounting rate\n",
        "\n",
        "# Exploration parameters\n",
        "epsilon = 1.0                 # Exploration rate\n",
        "max_epsilon = 1.0             # Exploration probability at start\n",
        "min_epsilon = 0.01            # Minimum exploration probability \n",
        "decay_rate = 0.01             # Exponential decay rate for exploration prob"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5d0E2-YTkAGz",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "aa0d5fe1-44cd-4152-c06b-9a7771c7a31b"
      },
      "source": [
        "for episode in range(total_episodes):\n",
        "    state = env.reset()\n",
        "    done = False\n",
        "    clear_output(wait = True)\n",
        "    print(f\"Episode = {episode + 1}/{total_episodes} \")\n",
        "    for step in range(max_steps):\n",
        "        #clear_output(wait=True)\n",
        "        \n",
        "        epsilon = min(min_epsilon, epsilon*decay_rate)\n",
        "        # kiểm tra xem agent dùng exploi hay explor\n",
        "        if np.random.rand() < epsilon:\n",
        "            # exploration\n",
        "            action = np.random.randint(0, action_size)\n",
        "        else:\n",
        "            # exploitation\n",
        "            if np.max(q_table[state]) == 0:\n",
        "                action=np.random.randint(0,action_size)\n",
        "            else:\n",
        "                action = np.argmax(q_table[state])\n",
        "            \n",
        "        # nhận reward và state tiếp theo\n",
        "        new_state, reward, done, _ = env.step(action)\n",
        "\n",
        "        # cập nhật q_table theo Bellman equation\n",
        "        update = reward + discount_rate*q_table[new_state].max() - q_table[state,action]\n",
        "        q_table[state,action] = q_table[state,action] + learning_rate*update\n",
        "        state = new_state\n",
        "        if done:\n",
        "            break"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Episode = 5000/5000 \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "voVUojvGkIFF"
      },
      "source": [
        "np.save(FILE_SAVE_Qlearning,q_table)"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1ZOiQvfokI7Z",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8439a50b-0cd2-477e-bdcb-0fc3877f4c63"
      },
      "source": [
        "# từ q_table bắt đầu chơi thử\n",
        "q_table = np.load(\"q_table_qlearning.npy\")\n",
        "rewards = []\n",
        "\n",
        "for episode in range(total_test_episodes):\n",
        "    state = env.reset()\n",
        "    step = 0\n",
        "    done = False\n",
        "    total_rewards = 0\n",
        "    \n",
        "\n",
        "    for step in range(max_steps):\n",
        "        print(\"****************************************************\")\n",
        "        print(\"EPISODE \", episode + 1)\n",
        "        \n",
        "        action = np.argmax(q_table[state,:])\n",
        "        new_state, reward, done, info = env.step(action)\n",
        "        env.render() # hiển thị trò chơi\n",
        "        total_rewards += reward\n",
        "        print(total_rewards)\n",
        "        clear_output(wait = True)\n",
        "        \n",
        "        if done:\n",
        "            rewards.append(total_rewards)\n",
        "            break\n",
        "        state = new_state\n",
        "        \n",
        "env.close()\n",
        "print (\"Score over time: \" +  str(sum(rewards)/total_test_episodes))"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Score over time: 7.82\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GTA6xgkYkXK6"
      },
      "source": [
        "# Bài tập\n",
        "1. Giải thích vì sao khi dùng Naive-Learning thì xe taxi chỉ đứng yên một chỗ?\n",
        "2. Giải thích vì sao khi dùng Q-Leaning thì xe taxi có thể đón và trả khách được?\n",
        "3. Tìm hiểu một game khác trên OpenAI và thiết lập cho agent chơi được"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. Giải thích vì sao khi dùng Naive-Learning thì xe taxi chỉ đứng yên một chỗ?\n",
        "**Giải thích:** Khi dùng Naive-Learning, trong một số trường hợp thì xe di chuyển một vài bước ban đầu rồi đứng yên một chỗ </br> Nguyên nhân là vì do cách học của Naive-Learning nên agent RL có xu hướng chọn hành động tốt nhất **tại thời điểm hiện tại**. Từ cách ta định nghĩa reward cho từng hành động\n",
        "\n",
        "*   ta luôn nhận -1 reward tại mỗi bước\n",
        "*   trả khách đúng nơi thì nhận +20 reward\n",
        "*   thực hiện đón/trả khách không hợp lệ thì nhận -10 reward\n",
        "\n",
        "Do đó, tại mỗi thời điểm, agent RL sẽ hạn chế/không thực hiện đón/trả khách vì hành động này có **khả năng cao** là không hợp lệ. </br>\n",
        "Vì thế, tại các bước ban đầu, nhằm tối đa reward nhận được, agent RL sẽ\n",
        "\n",
        "*   di chuyển tự do nếu nó không gần \"tường\" của môi trường (di chuyển 1 bước: -1 reward)\n",
        "*   \"di chuyển\" vào tường (không di chuyển: -1 reward)\n",
        "\n"
      ],
      "metadata": {
        "id": "PS_I_sMQjVaK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n"
      ],
      "metadata": {
        "id": "3jTKQTXB8LgD"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "2. Giải thích vì sao khi dùng Q-Learning thì xe taxi có thể đón và trả khách được ?\n",
        "**Giải thích:** Như câu 1, một hạn chế mà ta thấy của Naive-Learning là agent RL không biết được **tất cả** các hành động cũng như các reward tương ứng mà môi trường cung cấp </br>\n",
        "Q-Learning cho phép agent RL khám phá môi trường một cách ngẫu nhiên và do đó agent RL học được **tất cả (*hầu hết*)** các hành động/trạng thái và reward của môi trường. Hơn nữa, cách Q-learning chọn hành động cho bước tiếp theo có phần phụ thuộc vào tương lai $\\left(\\displaystyle\\max_{a}Q(s_{t+1}, a)\\right)$ thay vì chỉ phụ thuộc vào trạng thái hiện tại như Naive-Learning </br>\n",
        "Vì thế, agent RL của Q-learning hoạt động tốt hơn và có thể đón, trả khách được"
      ],
      "metadata": {
        "id": "QVRjzB7I5O60"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "jKHl97lZ8Og8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "3. Tìm hiểu một game khác trên OpenAI và thiết lập cho agent chơi được"
      ],
      "metadata": {
        "id": "TeabXaYP8Qfo"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Mô tả môi trường:**\n",
        "\n",
        "*   Trong môi trường FrozenLake, agent sẽ cố gắng đi từ điểm xuất phát (\"S\") đến điểm kết thúc (\"G\"), đường đi do trời lạnh nên đã bị đóng băng (\"F\") và tại một vài nơi thì băng rã và tạo ra hố (\"H\") \n",
        "*   Agent được thực hiện 4 hành động tại mỗi bước: sang trái, sang phải, lên trên và xuống dưới\n",
        "* Tại mỗi bước, agent không nhận bất kì reward nào, agent chỉ nhận reward = +1 khi agent đi đến điểm kết thúc (\"G\")\n",
        "* Một episode dừng khi agent rơi vào hố (\"H\") hoặc đi đến điểm kết thúc (\"G\")"
      ],
      "metadata": {
        "id": "5gmH8aaTmjls"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "env = gym.make(\"FrozenLake-v0\", map_name = \"8x8\", is_slippery = False)\n",
        "state_size = env.observation_space.n\n",
        "action_size = env.action_space.n\n",
        "\n",
        "q_table = np.zeros((state_size, action_size))\n",
        "\n",
        "FILE_SAVE_Qlearning = \"q_table_qlearning_frozen.npy\"\n",
        "total_episodes = 5000        # Total episodes\n",
        "total_test_episodes = 50     # Total test episodes\n",
        "max_steps = 100              # Max steps per episode\n",
        "\n",
        "learning_rate = 0.7          # Learning rate\n",
        "discount_rate = 0.4          # Discounting rate\n",
        "\n",
        "# Exploration parameters\n",
        "epsilon = 1.0                 # Exploration rate\n",
        "max_epsilon = 1.0             # Exploration probability at start\n",
        "min_epsilon = 1e-3            # Minimum exploration probability \n",
        "decay_rate = 0.9              # Exponential decay rate for exploration prob"
      ],
      "metadata": {
        "id": "8uZly7x0QWHv"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for episode in range(total_episodes):\n",
        "    state = env.reset()\n",
        "    done = False\n",
        "    clear_output(wait = True)\n",
        "    print(f\"Episode = {episode + 1}/{total_episodes} \")\n",
        "    for step in range(max_steps):\n",
        "        #clear_output(wait=True)\n",
        "        \n",
        "        epsilon = min(min_epsilon, epsilon*decay_rate)\n",
        "        # kiểm tra xem agent dùng exploit hay explore\n",
        "        if np.random.uniform() < epsilon:\n",
        "            # exploration\n",
        "            action = np.random.randint(0, action_size)\n",
        "        else:\n",
        "            # exploitation\n",
        "            if np.max(q_table[state]) == 0:\n",
        "                action = np.random.randint(0,action_size)\n",
        "            else:\n",
        "                action = np.argmax(q_table[state])\n",
        "            \n",
        "        # nhận reward và state tiếp theo\n",
        "        new_state, reward, done, _ = env.step(action)\n",
        "\n",
        "        # cập nhật q_table theo Bellman equation\n",
        "        update = reward + discount_rate*q_table[new_state].max() - q_table[state,action]\n",
        "        q_table[state,action] = q_table[state,action] + learning_rate*update\n",
        "        state = new_state\n",
        "        if done:\n",
        "            break"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "twu68fkgQaqP",
        "outputId": "2a7ba257-0508-4f6c-9f2d-e027c658a386"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Episode = 5000/5000 \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "np.save(FILE_SAVE_Qlearning,q_table)"
      ],
      "metadata": {
        "id": "rILx3qTdQu0v"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# từ q_table bắt đầu chơi thử\n",
        "q_table = np.load(\"q_table_qlearning_frozen.npy\")\n",
        "rewards = []\n",
        "\n",
        "for episode in range(total_test_episodes):\n",
        "    state = env.reset()\n",
        "    step = 0\n",
        "    done = False\n",
        "    total_rewards = 0\n",
        "    \n",
        "\n",
        "    for step in range(max_steps):\n",
        "        print(\"****************************************************\")\n",
        "        print(\"EPISODE \", episode + 1)\n",
        "        \n",
        "        action = np.argmax(q_table[state,:])\n",
        "        new_state, reward, done, info = env.step(action)\n",
        "        env.render() # hiển thị trò chơi\n",
        "        total_rewards += reward\n",
        "        print(total_rewards)\n",
        "        time.sleep(0.5)\n",
        "        clear_output(wait = True)\n",
        "        \n",
        "        if done:\n",
        "            rewards.append(total_rewards)\n",
        "            break\n",
        "        state = new_state\n",
        "        \n",
        "env.close()\n",
        "print (\"Score over time: \" +  str(sum(rewards)/total_test_episodes))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PcRTTb1vQ4kw",
        "outputId": "38a41388-4e58-49ed-af9e-ffc3f1784c93"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Score over time: 1.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "q_table = np.load(\"q_table_qlearning_frozen.npy\")\n",
        "\n",
        "state = env.reset()\n",
        "env.render()\n",
        "done = False\n",
        "\n",
        "for step in range(14): # In thử 14 step của 1 episode\n",
        "    print(\"*\"*50)\n",
        "    action = np.argmax(q_table[state,:])\n",
        "    new_state, reward, done, info = env.step(action)\n",
        "    env.render() # hiển thị trò chơi\n",
        "    time.sleep(0.5)\n",
        "    \n",
        "    if done:\n",
        "        break\n",
        "    state = new_state\n",
        "        \n",
        "env.close()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "f64XTPe25-na",
        "outputId": "70cfcb08-b17a-445c-f47d-744bfdb99a17"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\u001b[41mS\u001b[0mFFFFFFF\n",
            "FFFFFFFF\n",
            "FFFHFFFF\n",
            "FFFFFHFF\n",
            "FFFHFFFF\n",
            "FHHFFFHF\n",
            "FHFFHFHF\n",
            "FFFHFFFG\n",
            "**************************************************\n",
            "  (Right)\n",
            "S\u001b[41mF\u001b[0mFFFFFF\n",
            "FFFFFFFF\n",
            "FFFHFFFF\n",
            "FFFFFHFF\n",
            "FFFHFFFF\n",
            "FHHFFFHF\n",
            "FHFFHFHF\n",
            "FFFHFFFG\n",
            "**************************************************\n",
            "  (Right)\n",
            "SF\u001b[41mF\u001b[0mFFFFF\n",
            "FFFFFFFF\n",
            "FFFHFFFF\n",
            "FFFFFHFF\n",
            "FFFHFFFF\n",
            "FHHFFFHF\n",
            "FHFFHFHF\n",
            "FFFHFFFG\n",
            "**************************************************\n",
            "  (Right)\n",
            "SFF\u001b[41mF\u001b[0mFFFF\n",
            "FFFFFFFF\n",
            "FFFHFFFF\n",
            "FFFFFHFF\n",
            "FFFHFFFF\n",
            "FHHFFFHF\n",
            "FHFFHFHF\n",
            "FFFHFFFG\n",
            "**************************************************\n",
            "  (Right)\n",
            "SFFF\u001b[41mF\u001b[0mFFF\n",
            "FFFFFFFF\n",
            "FFFHFFFF\n",
            "FFFFFHFF\n",
            "FFFHFFFF\n",
            "FHHFFFHF\n",
            "FHFFHFHF\n",
            "FFFHFFFG\n",
            "**************************************************\n",
            "  (Right)\n",
            "SFFFF\u001b[41mF\u001b[0mFF\n",
            "FFFFFFFF\n",
            "FFFHFFFF\n",
            "FFFFFHFF\n",
            "FFFHFFFF\n",
            "FHHFFFHF\n",
            "FHFFHFHF\n",
            "FFFHFFFG\n",
            "**************************************************\n",
            "  (Right)\n",
            "SFFFFF\u001b[41mF\u001b[0mF\n",
            "FFFFFFFF\n",
            "FFFHFFFF\n",
            "FFFFFHFF\n",
            "FFFHFFFF\n",
            "FHHFFFHF\n",
            "FHFFHFHF\n",
            "FFFHFFFG\n",
            "**************************************************\n",
            "  (Down)\n",
            "SFFFFFFF\n",
            "FFFFFF\u001b[41mF\u001b[0mF\n",
            "FFFHFFFF\n",
            "FFFFFHFF\n",
            "FFFHFFFF\n",
            "FHHFFFHF\n",
            "FHFFHFHF\n",
            "FFFHFFFG\n",
            "**************************************************\n",
            "  (Down)\n",
            "SFFFFFFF\n",
            "FFFFFFFF\n",
            "FFFHFF\u001b[41mF\u001b[0mF\n",
            "FFFFFHFF\n",
            "FFFHFFFF\n",
            "FHHFFFHF\n",
            "FHFFHFHF\n",
            "FFFHFFFG\n",
            "**************************************************\n",
            "  (Down)\n",
            "SFFFFFFF\n",
            "FFFFFFFF\n",
            "FFFHFFFF\n",
            "FFFFFH\u001b[41mF\u001b[0mF\n",
            "FFFHFFFF\n",
            "FHHFFFHF\n",
            "FHFFHFHF\n",
            "FFFHFFFG\n",
            "**************************************************\n",
            "  (Down)\n",
            "SFFFFFFF\n",
            "FFFFFFFF\n",
            "FFFHFFFF\n",
            "FFFFFHFF\n",
            "FFFHFF\u001b[41mF\u001b[0mF\n",
            "FHHFFFHF\n",
            "FHFFHFHF\n",
            "FFFHFFFG\n",
            "**************************************************\n",
            "  (Right)\n",
            "SFFFFFFF\n",
            "FFFFFFFF\n",
            "FFFHFFFF\n",
            "FFFFFHFF\n",
            "FFFHFFF\u001b[41mF\u001b[0m\n",
            "FHHFFFHF\n",
            "FHFFHFHF\n",
            "FFFHFFFG\n",
            "**************************************************\n",
            "  (Down)\n",
            "SFFFFFFF\n",
            "FFFFFFFF\n",
            "FFFHFFFF\n",
            "FFFFFHFF\n",
            "FFFHFFFF\n",
            "FHHFFFH\u001b[41mF\u001b[0m\n",
            "FHFFHFHF\n",
            "FFFHFFFG\n",
            "**************************************************\n",
            "  (Down)\n",
            "SFFFFFFF\n",
            "FFFFFFFF\n",
            "FFFHFFFF\n",
            "FFFFFHFF\n",
            "FFFHFFFF\n",
            "FHHFFFHF\n",
            "FHFFHFH\u001b[41mF\u001b[0m\n",
            "FFFHFFFG\n",
            "**************************************************\n",
            "  (Down)\n",
            "SFFFFFFF\n",
            "FFFFFFFF\n",
            "FFFHFFFF\n",
            "FFFFFHFF\n",
            "FFFHFFFF\n",
            "FHHFFFHF\n",
            "FHFFHFHF\n",
            "FFFHFFF\u001b[41mG\u001b[0m\n"
          ]
        }
      ]
    }
  ]
}